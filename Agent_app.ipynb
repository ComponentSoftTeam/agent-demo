{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from datetime import datetime\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "News_api_key = os.environ[\"NEWS_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_var_in_env_file(var_name, env_file_path='.env'):\n",
    "    \"\"\"Check if a variable exists in a .env file\"\"\"\n",
    "    var_exists = False\n",
    "    if os.path.exists(env_file_path):\n",
    "        with open(env_file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.startswith(var_name):\n",
    "                    var_exists = True\n",
    "                    break\n",
    "    return var_exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if check_var_in_env_file('GCP'):\n",
    "    !gcloud auth application-default login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "#from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from local_google_genai_chat_models import ChatGoogleGenerativeAI\n",
    "#from langchain_google_vertexai import ChatVertexAI\n",
    "from local_google_vertexai_chat_models import ChatVertexAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from langchain_fireworks.chat_models import ChatFireworks\n",
    "from langchain_core.output_parsers.string import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "from langchain_community.utilities.duckduckgo_search import DuckDuckGoSearchAPIWrapper\n",
    "\n",
    "#from langchain.agents import create_tool_calling_agent, create_react_agent, AgentExecutor\n",
    "from langchain.agents import create_tool_calling_agent, create_react_agent\n",
    "from local_agent import AgentExecutor\n",
    "#from langchain_community.agent_toolkits.load_tools import load_tools\n",
    "from local_load_tools import load_tools\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.globals import set_debug, set_verbose\n",
    "from langchain_core.output_parsers.string import StrOutputParser\n",
    "from langchain import hub\n",
    "\n",
    "set_debug(False)\n",
    "set_verbose(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPERATURE = 0.0\n",
    "MAX_NEW_TOKENS = 1000\n",
    "\n",
    "LLM_MODELS = {\n",
    "        \"llama-3-70b-prompting\": ChatFireworks(\n",
    "            model_name=\"accounts/fireworks/models/llama-v3-70b-instruct\",\n",
    "            temperature=TEMPERATURE,\n",
    "            max_tokens=MAX_NEW_TOKENS,            \n",
    "        ),    \n",
    "        \"Llama-3-70b-firefunction-v2\": ChatFireworks(\n",
    "            model_name=\"accounts/fireworks/models/firefunction-v1\",\n",
    "            temperature=TEMPERATURE,\n",
    "            max_tokens=MAX_NEW_TOKENS,            \n",
    "        ),\n",
    "        \"gpt-3.5-turbo\": ChatOpenAI(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            temperature=TEMPERATURE,\n",
    "            max_tokens=MAX_NEW_TOKENS,\n",
    "        ),    \n",
    "        \"gpt-4o\": ChatOpenAI(\n",
    "            model=\"gpt-4o\",\n",
    "            temperature=TEMPERATURE,\n",
    "            max_tokens=MAX_NEW_TOKENS,\n",
    "        ),\n",
    "        \"gpt-4-turbo\": ChatOpenAI(\n",
    "            model=\"gpt-4-turbo\",\n",
    "            temperature=TEMPERATURE,\n",
    "            max_tokens=MAX_NEW_TOKENS,\n",
    "        ),        \n",
    "        \"mistral-large\": ChatMistralAI(\n",
    "            model=\"mistral-large-latest\",\n",
    "            temperature=TEMPERATURE,\n",
    "            max_tokens=MAX_NEW_TOKENS,\n",
    "        ),\n",
    "        \"open-mixtral-8x22b\": ChatMistralAI(\n",
    "            model=\"open-mixtral-8x22b\",\n",
    "            temperature=TEMPERATURE,\n",
    "            max_tokens=MAX_NEW_TOKENS,\n",
    "        ),\n",
    "        \"mistral-small\": ChatMistralAI(\n",
    "            model=\"mistral-small-latest\",\n",
    "            temperature=TEMPERATURE,\n",
    "            max_tokens=MAX_NEW_TOKENS,\n",
    "        ),        \n",
    "        \"gemini-1.5-flash\": ChatGoogleGenerativeAI(\n",
    "        #\"gemini-1.5-flash\": ChatVertexAI(\n",
    "            model=\"gemini-1.5-flash-latest\",\n",
    "            temperature=TEMPERATURE,\n",
    "            max_tokens=MAX_NEW_TOKENS,\n",
    "        ),\n",
    "        \"gemini-1.5-pro\": ChatGoogleGenerativeAI(\n",
    "        #\"gemini-1.5-pro\": ChatVertexAI(\n",
    "            model=\"gemini-1.5-pro-latest\",\n",
    "            temperature=TEMPERATURE,\n",
    "            max_tokens=MAX_NEW_TOKENS,\n",
    "        ),\n",
    "        \"claude-3-haiku\": ChatAnthropic(\n",
    "            model=\"claude-3-haiku-20240307\",\n",
    "            temperature=TEMPERATURE,\n",
    "            max_tokens=MAX_NEW_TOKENS,\n",
    "        ),    \n",
    "        \"claude-3.5-sonnet\": ChatAnthropic(\n",
    "            model=\"claude-3-5-sonnet-20240620\",\n",
    "            temperature=TEMPERATURE,\n",
    "            max_tokens=MAX_NEW_TOKENS,\n",
    "        ),\n",
    "        \"claude-3-opus\": ChatAnthropic(\n",
    "            model=\"claude-3-opus-20240229\",\n",
    "            temperature=TEMPERATURE,\n",
    "            max_tokens=MAX_NEW_TOKENS,\n",
    "        ),\n",
    "    }\n",
    "\n",
    "model_type = \"gemini-1.5-pro\"\n",
    "llm = LLM_MODELS[model_type]\n",
    "\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DATE = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "llm_math_tool, weather_tool, news_tool, wikipedia_tool, arxiv_tool, websearch_tool = load_tools([\"llm-math\", \"open-meteo-api\", \"news-api\", \"wikipedia\", \"arxiv\", \"ddg-search\"], news_api_key=News_api_key, llm=llm)\n",
    "\n",
    "#llm_math_tool.invoke(\"What is the square root of 4?\")\n",
    "#wikipedia_tool.invoke(\"How many people live in Prague?\")\n",
    "#print(weather_tool.invoke(prompt, verbose=False))\n",
    "#print(news_tool.invoke(\"What are the 10 most important news in Prague? Answer in 10 bullet points\"))\n",
    "#print(arxiv_tool.invoke(\"List the title of 10 scientific papers about LLM agents published in this year.\", verbose=True))\n",
    "#print(websearch_tool.invoke(\"Who won the most Oscar in this year?\"))\n",
    "#print(pubmed_tool.invoke(\"List papers about Vortioxetin.\"))llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools=[llm_math_tool, arxiv_tool, news_tool, weather_tool, wikipedia_tool, websearch_tool] # More tools could be added\n",
    "\n",
    "if model_type.startswith(\"llama\"):\n",
    "    prompt_react = hub.pull(\"hwchase17/react\")\n",
    "    agent = create_react_agent(llm, tools, prompt_react)\n",
    "else:\n",
    "    prompt_tool_calling = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"{system_prompt}\"),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "    ])        \n",
    "    agent = create_tool_calling_agent(llm, tools, prompt_tool_calling)\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose = True, stream_runnable=False)\n",
    "# , enable_automatic_function_calling=True\n",
    "\n",
    "agent_chain = agent_executor | itemgetter(\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f\"You're a helpful assistant. Always use tools to answer questions. Always use the Calculator for calculations, even when adding 2 numbers or calculating averages. The current date is {datetime.today().strftime('%Y-%m-%d')}\"\n",
    "\n",
    "#prompt = \"What is the square root of 2?\"\n",
    "#prompt = \"What is the weather like in Budapest?\"\n",
    "#prompt = \"What are the 10 trending news in Budapest Hungary? Answer in 10 bullet points.\"\n",
    "#prompt = \"How many people live in Budapest?\"\n",
    "#prompt = \"Who won the most Oscar in this year?\"\n",
    "#prompt = \"List the title of 10 scientific papers about LLM agents published in this year.\"\n",
    "\n",
    "#prompt = \"What are the maximum, minimum and average temperature values as well as the sum of precipitations on each of the coming 3 days in Budapest?\"\n",
    "#prompt = f\"Create a table that contains the date as well as the maximum, minimum and average temperature values as well as the sum of precipitations in the coming 7 days in Budapest\"\n",
    "prompt = f\"\"\"Create a table that contains the date as well as the maximum, minimum and average temperature values as well as the sum of precipitations separately for each of the coming 7 days in Budapest.\n",
    "Then include a line with each of the average of the maximum, minimum and average temperature values for these 7 days. And after all that also write a list with 10 current news in Budapest Hungary.\"\"\"\n",
    "\n",
    "prompt = \"What is the square root of 2?\"\n",
    "streaming = False\n",
    "\n",
    "response = \"\"\n",
    "if streaming == True:\n",
    "    for new_token in agent_chain.stream({\"input\": prompt}, {\"callbacks\": [VariableCallbackHandler(session_id)]}):\n",
    "        print(new_token, end=\"\")\n",
    "        response = response + new_token\n",
    "else:\n",
    "    response = agent_chain.invoke({\"system_prompt\": system_prompt, \"input\": prompt})\n",
    "    print(response)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agent_executor.invoke({\"input\": \"How many people live in Budapest and Prague together?\"})\n",
    "print(agent_chain.invoke({\"system_prompt\": system_prompt, \"input\": \"What is the square root of 2?\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent_chain.invoke({\"system_prompt\": system_prompt, \"input\": \"What is the weather like in Budapest?\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(agent_chain.invoke({\"system_prompt\": system_prompt, \"input\": \"What are the 10 trending news in Budapest Hungary? Answer in 10 bullet points.\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent_chain.invoke({\"system_prompt\": system_prompt, \"input\": \"How many people live in Budapest?\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent_chain.invoke({\"system_prompt\": system_prompt, \"input\": \"Who won the most Oscar in this year?\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent_chain.invoke({\"system_prompt\": system_prompt, \"input\": \"List the title of 10 scientific papers about LLM agents published in this year.\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent_chain.invoke({\"system_prompt\": system_prompt, \"input\": \"What are the maximum, minimum and average temperature values as well as the sum of precipitations on each of the coming 3 days in Budapest?\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent_chain.invoke({\"system_prompt\": system_prompt, \"input\": \"Create a table that contains the date as well as the maximum, minimum and average temperature values as well as the sum of precipitations in the coming 7 days in Budapest?\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent_chain.invoke({\"system_prompt\": system_prompt, \"input\": \"\"\"Create a table that contains the date as well as the maximum, minimum and average temperature values as well as the sum of precipitations separately for each of the coming 7 days in Budapest.\n",
    "Then include a line with each of the average of the maximum, minimum and avarage temperature values for these 7 days. And after all that also write a list with 10 current news in Budapest Hungary.\"\"\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
