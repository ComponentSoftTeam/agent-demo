{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from operator import itemgetter\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "import os\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Agents with Langchain\"\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "News_api_key = os.environ[\"NEWS_API_KEY\"]\n",
    "Financial_news_api_key=os.environ[\"ALPHAVANTAGE_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from langchain_fireworks.chat_models import ChatFireworks\n",
    "from langchain_core.output_parsers.string import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPERATURE = 0.02\n",
    "MAX_NEW_TOKENS = 4000\n",
    "\n",
    "MODELS = {\n",
    "    \"llama-2\": ChatFireworks(\n",
    "        model_name=\"accounts/fireworks/models/llama-v2-70b-chat\",\n",
    "        temperature=TEMPERATURE,\n",
    "        max_tokens=MAX_NEW_TOKENS,            \n",
    "    ),\n",
    "    \"llama-3\": ChatFireworks(\n",
    "        model_name=\"accounts/fireworks/models/llama-v3-70b-instruct\",\n",
    "        temperature=TEMPERATURE,\n",
    "        max_tokens=MAX_NEW_TOKENS,            \n",
    "    ),    \n",
    "    \"firefunction\": ChatFireworks(\n",
    "        model_name=\"accounts/fireworks/models/firefunction-v1\",\n",
    "        temperature=TEMPERATURE,\n",
    "        max_tokens=MAX_NEW_TOKENS,            \n",
    "    ),\n",
    "    \"gpt3_5turbo\": ChatOpenAI(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        temperature=TEMPERATURE,\n",
    "        max_tokens=MAX_NEW_TOKENS,\n",
    "    ),    \n",
    "    \"gpt4o\": ChatOpenAI(\n",
    "        model=\"gpt-4o\",\n",
    "        temperature=TEMPERATURE,\n",
    "        max_tokens=MAX_NEW_TOKENS,\n",
    "    ),\n",
    "    \"mistral\": ChatMistralAI(\n",
    "        model=\"mistral-large-latest\",\n",
    "        temperature=TEMPERATURE,\n",
    "        max_tokens=MAX_NEW_TOKENS,\n",
    "    ),\n",
    "    \"gemini_flash\": ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-1.5-flash-latest\",\n",
    "        temperature=TEMPERATURE,\n",
    "        max_tokens=MAX_NEW_TOKENS,\n",
    "    ),\n",
    "    \"gemini_pro\": ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-1.5-pro-latest\",\n",
    "        temperature=TEMPERATURE,\n",
    "        max_tokens=MAX_NEW_TOKENS,\n",
    "    ),\n",
    "}\n",
    "\n",
    "router_llm = MODELS[\"gpt4o\"]\n",
    "tool_llm = MODELS[\"gpt4o\"]\n",
    "\n",
    "# Model output -> string\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents with vendor specific function calling APIs\n",
    "\n",
    "Let's create a new agent, that uses the vendors implementation of the function calling api.\n",
    "\n",
    "Like OpenAI functions\n",
    "\n",
    "Also take a look at the llms and vendors that support function calling apis [here](https://python.langchain.com/docs/integrations/chat/?ref=blog.langchain.dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain.tools import WikipediaQueryRun\n",
    "\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "from langchain_community.utilities.duckduckgo_search import DuckDuckGoSearchAPIWrapper\n",
    "#from langchain_community.tools.ddg_search.tool import DuckDuckGoSearchRun\n",
    "from langchain_community.tools.pubmed.tool import PubmedQueryRun\n",
    "\n",
    "from langchain_community.utilities.alpha_vantage import AlphaVantageAPIWrapper\n",
    "from langchain.agents import create_tool_calling_agent, AgentExecutor, load_tools\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.globals import set_debug, set_verbose\n",
    "from langchain_core.output_parsers.string import StrOutputParser\n",
    "\n",
    "set_debug(False)\n",
    "set_verbose(False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "alpha_vantage = AlphaVantageAPIWrapper()\n",
    "#print(alpha_vantage._get_exchange_rate(\"EUR\", \"HUF\"))\n",
    "display(alpha_vantage._get_market_news_sentiment(\"MNST\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create a table that contains the date as well as the maximum, minimum and avarage temperature values as well as the sum of precipitations separately for each of the coming 7 days in Budapest\n"
     ]
    }
   ],
   "source": [
    "DATE = datetime.today().strftime('%Y-%m-%d')\n",
    "#prompt = f\"Create a table that contains the date as well as the maximum, minimum and avarage temperature values separately for each of 2024-05-18, 2024-05-19 and 2024-05-20 in Budapest.\"\n",
    "prompt = f\"Create a table that contains the date as well as the maximum, minimum and avarage temperature values as well as the sum of precipitations separately for each of the coming 7 days in Budapest\"\n",
    "print(prompt)\n",
    "\n",
    "#pubmed_tool = PubmedQueryRun()\n",
    "\n",
    "#llm_math_tool, wikipedia_tool, weather_tool, news_tool = load_tools([\"llm-math\",\"wikipedia\", \"open-meteo-api\", \"news-api\"], news_api_key=News_api_key, llm=llm)\n",
    "llm_math_tool, websearch_tool, wikipedia_tool, arxiv_tool, weather_tool, news_tool = load_tools([\"llm-math\",\"ddg-search\", \"wikipedia\", \"arxiv\", \"open-meteo-api\", \"news-api\"], news_api_key=News_api_key, llm=tool_llm)\n",
    "\n",
    "#llm_math_tool.invoke(\"What is the square root of 4?\")\n",
    "#wikipedia_tool.invoke(\"How many people live in Prague?\")\n",
    "#print()\n",
    "#print(weather_tool.invoke(prompt, verbose=True))\n",
    "#print(\"\\n\\n\")\n",
    "#print(news_tool.invoke(\"What are the 10 most important news in Prague? Answer in 10 bullet points\"))\n",
    "#print(arxiv_tool.invoke(\"List the title of 10 scientific papers about LLM agents published in this year.\", verbose=True))\n",
    "#print(websearch_tool.invoke(\"Who won the most Oscar in this year?\"))\n",
    "#print(pubmed_tool.invoke(\"List papers about Vortioxetin.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Callback Handler that writes to a variable.\"\"\"\n",
    "\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import TYPE_CHECKING, Any, Dict, Optional\n",
    "\n",
    "from langchain_core.callbacks.base import BaseCallbackHandler\n",
    "#from langchain_core.utils import print_text\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from langchain_core.agents import AgentAction, AgentFinish\n",
    "\n",
    "\n",
    "_TEXT_COLOR_MAPPING = {\n",
    "    \"blue\": \"36;1\",\n",
    "    \"yellow\": \"33;1\",\n",
    "    \"pink\": \"38;5;200\",\n",
    "    \"green\": \"32;1\",\n",
    "    \"red\": \"31;1\",\n",
    "}\n",
    "\n",
    "def get_color_mapping(\n",
    "    items: List[str], excluded_colors: Optional[List] = None\n",
    ") -> Dict[str, str]:\n",
    "    \"\"\"Get mapping for items to a support color.\"\"\"\n",
    "    colors = list(_TEXT_COLOR_MAPPING.keys())\n",
    "    if excluded_colors is not None:\n",
    "        colors = [c for c in colors if c not in excluded_colors]\n",
    "    color_mapping = {item: colors[i % len(colors)] for i, item in enumerate(items)}\n",
    "    return color_mapping\n",
    "\n",
    "\n",
    "def get_colored_text(text: str, color: str) -> str:\n",
    "    \"\"\"Get colored text.\"\"\"\n",
    "    color_str = _TEXT_COLOR_MAPPING[color]\n",
    "    return f\"\\u001b[{color_str}m\\033[1;3m{text}\\u001b[0m\"\n",
    "\n",
    "\n",
    "def get_bolded_text(text: str) -> str:\n",
    "    \"\"\"Get bolded text.\"\"\"\n",
    "    return f\"\\033[1m{text}\\033[0m\"\n",
    "\n",
    "\n",
    "def print_text(\n",
    "    text: str, color: Optional[str] = None, end: str = \"\"\n",
    ") -> str:\n",
    "    \"\"\"Print text with highlighting and no end characters.\"\"\"\n",
    "    #print(f\"color = {color}\")\n",
    "    text_to_print = get_colored_text(text, color) if color else text\n",
    "    #print(text_to_print, end=end)  # noqa: T201\n",
    "    return text_to_print\n",
    "\n",
    "class VariableCallbackHandler(BaseCallbackHandler):\n",
    "    \"\"\"Callback Handler that prints to a variable.\"\"\"\n",
    "\n",
    "    def __init__(self, color: Optional[str] = None) -> None:\n",
    "        \"\"\"Initialize callback handler.\"\"\"\n",
    "        self.color = \"blue\"\n",
    "\n",
    "    def on_chain_start(\n",
    "        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any\n",
    "    ) -> None:\n",
    "        \"\"\"Print out that we are entering a chain.\"\"\"\n",
    "        class_name = serialized.get(\"name\", serialized.get(\"id\", [\"<unknown>\"])[-1])\n",
    "        #print(f\"\\n\\n\\033[1m> Entering new {class_name} chain...\\033[0m\")  # noqa: T201\n",
    "        trace_list.append(f\"\\033[1m> Entering new {class_name} chain...\\033[0m\")\n",
    "\n",
    "    def on_chain_end(self, outputs: Dict[str, Any], **kwargs: Any) -> None:\n",
    "        \"\"\"Print out that we finished a chain.\"\"\"\n",
    "        #print(\"\\n\\033[1m> Finished chain.\\033[0m\")  # noqa: T201\n",
    "        trace_list.append(f\"\\033[1m> Finished chain.\\033[0m\")\n",
    "        \n",
    "    def on_agent_action(\n",
    "        self, action: AgentAction, color: Optional[str] = None, **kwargs: Any\n",
    "    ) -> Any:\n",
    "        \"\"\"Run on agent action.\"\"\"\n",
    "        #print_text(action.log, color=color or self.color)\n",
    "        new_text = print_text(action.log, color=color or self.color)\n",
    "        trace_list.append(new_text)\n",
    "\n",
    "    def on_tool_end(\n",
    "        self,\n",
    "        output: Any,\n",
    "        color: Optional[str] = None,\n",
    "        observation_prefix: Optional[str] = None,\n",
    "        llm_prefix: Optional[str] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        \"\"\"If not the final action, print out observation.\"\"\"\n",
    "        output = str(output)\n",
    "        if observation_prefix is not None:\n",
    "            #print_text(f\"\\n{observation_prefix}\")\n",
    "            trace_list.append(f\"{observation_prefix}\")\n",
    "        if llm_prefix is not None:\n",
    "            #print_text(f\"\\n{llm_prefix}\")\n",
    "            trace_list.append(f\"{llm_prefix}\")\n",
    "\n",
    "    def on_text(\n",
    "        self,\n",
    "        text: str,\n",
    "        color: Optional[str] = None,\n",
    "        end: str = \"\",\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        \"\"\"Run when agent ends.\"\"\"\n",
    "        #print_text(text, color=color or self.color, end=end)\n",
    "        new_text = print_text(text, color=color or self.color, end=end)\n",
    "        trace_list.append(new_text)\n",
    "\n",
    "    def on_agent_finish(\n",
    "        self, finish: AgentFinish, color: Optional[str] = None, **kwargs: Any\n",
    "    ) -> None:\n",
    "        \"\"\"Run on agent end.\"\"\"\n",
    "        #print_text(finish.log, color=color or self.color, end=\"\\n\")\n",
    "        new_text = print_text(finish.log, color=color or self.color, end=\"\")\n",
    "        trace_list.append(new_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain_core.callbacks import FileCallbackHandler\n",
    "#filecallhandler = FileCallbackHandler(filename=\"callback.txt\", mode=\"w\", color=\"red\")\n",
    "trace_list = []\n",
    "varcallhandler = VariableCallbackHandler()\n",
    "\n",
    "#tools=[weather_tool]\n",
    "tools=[llm_math_tool, arxiv_tool, news_tool, weather_tool, wikipedia_tool, websearch_tool] # More tools could be added\n",
    "# Be careful with older tools, they might break with newer models\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", f\"You're a helpful assistant. Always use tools to answer questions. Always use llm_math_tool for calculations, even when adding 2 numbers or calculating averages. The current date is {DATE}\"),\n",
    "    (\"placeholder\", \"{chat_history}\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "])\n",
    "agent = create_tool_calling_agent(router_llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, callbacks=[varcallhandler], verbose = False)\n",
    "# , enable_automatic_function_calling=True\n",
    "\n",
    "agent_chain = agent_executor | itemgetter(\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the weather details for Budapest over the coming 7 days:\n",
      "\n",
      "- **May 22, 2024**: \n",
      "  - Max Temp: 21.1°C\n",
      "  - Min Temp: 16.1°C\n",
      "  - Precipitation: 3.10 mm\n",
      "\n",
      "- **May 23, 2024**: \n",
      "  - Max Temp: 24.5°C\n",
      "  - Min Temp: 14.8°C\n",
      "  - Precipitation: 7.30 mm\n",
      "\n",
      "- **May 24, 2024**: \n",
      "  - Max Temp: 20.5°C\n",
      "  - Min Temp: 16.5°C\n",
      "  - Precipitation: 13.70 mm\n",
      "\n",
      "- **May 25, 2024**: \n",
      "  - Max Temp: 24.9°C\n",
      "  - Min Temp: 14.9°C\n",
      "  - Precipitation: 0.30 mm\n",
      "\n",
      "- **May 26, 2024**: \n",
      "  - Max Temp: 26.1°C\n",
      "  - Min Temp: 16.5°C\n",
      "  - Precipitation: 0.00 mm\n",
      "\n",
      "- **May 27, 2024**: \n",
      "  - Max Temp: 27.3°C\n",
      "  - Min Temp: 17.0°C\n",
      "  - Precipitation: 0.00 mm\n",
      "\n",
      "- **May 28, 2024**: \n",
      "  - Max Temp: 27.2°C\n",
      "  - Min Temp: 16.5°C\n",
      "  - Precipitation: 0.00 mm\n",
      "\n",
      "### Summary:\n",
      "- **Average Maximum Temperature**: 24.51°C\n",
      "- **Average Minimum Temperature**: 16.04°C\n",
      "- **Total Precipitation**: 24.40 mm\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What are the maximum, minimum and average temperature values as well as the sum of precipitations on each of the coming 3 days in Budapest?\"\n",
    "#prompt = f\"Create a table that contains the date as well as the maximum, minimum and average temperature values as well as the sum of precipitations in the coming 7 days in Budapest\"\n",
    "#prompt = f\"\"\"Create a table that contains the date as well as the maximum, minimum and average temperature values as well as the sum of precipitations separately for each of the coming 7 days in Budapest.\n",
    "#Then include a line with each of the average of the maximum, minimum and avarage temperature values for these 7 days. And after all that also write a list with 10 current news in Budapest Hungary.\"\"\"\n",
    "\n",
    "trace_list = []\n",
    "streaming = True\n",
    "\n",
    "response = \"\"\n",
    "if streaming == True:\n",
    "    for new_token in agent_chain.stream({\"input\": prompt}):\n",
    "        #if new_token is not None:\n",
    "        print(new_token, end=\"\")\n",
    "        response = response + new_token\n",
    "else:\n",
    "    response = agent_chain.invoke({\"input\": prompt})\n",
    "    print(response)\n",
    "print(\"\")\n",
    "\n",
    "#response = agent_chain.stream({\"input\": prompt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the weather details for Budapest over the coming 7 days:\n",
      "\n",
      "- **May 22, 2024**: \n",
      "  - Max Temp: 21.1°C\n",
      "  - Min Temp: 16.1°C\n",
      "  - Precipitation: 3.10 mm\n",
      "\n",
      "- **May 23, 2024**: \n",
      "  - Max Temp: 24.5°C\n",
      "  - Min Temp: 14.8°C\n",
      "  - Precipitation: 7.30 mm\n",
      "\n",
      "- **May 24, 2024**: \n",
      "  - Max Temp: 20.5°C\n",
      "  - Min Temp: 16.5°C\n",
      "  - Precipitation: 13.70 mm\n",
      "\n",
      "- **May 25, 2024**: \n",
      "  - Max Temp: 24.9°C\n",
      "  - Min Temp: 14.9°C\n",
      "  - Precipitation: 0.30 mm\n",
      "\n",
      "- **May 26, 2024**: \n",
      "  - Max Temp: 26.1°C\n",
      "  - Min Temp: 16.5°C\n",
      "  - Precipitation: 0.00 mm\n",
      "\n",
      "- **May 27, 2024**: \n",
      "  - Max Temp: 27.3°C\n",
      "  - Min Temp: 17.0°C\n",
      "  - Precipitation: 0.00 mm\n",
      "\n",
      "- **May 28, 2024**: \n",
      "  - Max Temp: 27.2°C\n",
      "  - Min Temp: 16.5°C\n",
      "  - Precipitation: 0.00 mm\n",
      "\n",
      "### Summary:\n",
      "- **Average Maximum Temperature**: 24.51°C\n",
      "- **Average Minimum Temperature**: 16.04°C\n",
      "- **Total Precipitation**: 24.40 mm\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `OpenMeteoAPI` with `What are the maximum, minimum and average temperature values as well as the sum of precipitations on each of the coming 7 days in Budapest?`\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `Calculator` with `(21.1 + 24.5 + 20.5 + 24.9 + 26.1 + 27.3 + 27.2) / 7`\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `Calculator` with `(16.1 + 14.8 + 16.5 + 14.9 + 16.5 + 17.0 + 16.5) / 7`\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `Calculator` with `3.10 + 7.30 + 13.70 + 0.30 + 0.00 + 0.00 + 0.00`\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mHere are the weather details for Budapest over the coming 7 days:\n",
      "\n",
      "- **May 22, 2024**: \n",
      "  - Max Temp: 21.1°C\n",
      "  - Min Temp: 16.1°C\n",
      "  - Precipitation: 3.10 mm\n",
      "\n",
      "- **May 23, 2024**: \n",
      "  - Max Temp: 24.5°C\n",
      "  - Min Temp: 14.8°C\n",
      "  - Precipitation: 7.30 mm\n",
      "\n",
      "- **May 24, 2024**: \n",
      "  - Max Temp: 20.5°C\n",
      "  - Min Temp: 16.5°C\n",
      "  - Precipitation: 13.70 mm\n",
      "\n",
      "- **May 25, 2024**: \n",
      "  - Max Temp: 24.9°C\n",
      "  - Min Temp: 14.9°C\n",
      "  - Precipitation: 0.30 mm\n",
      "\n",
      "- **May 26, 2024**: \n",
      "  - Max Temp: 26.1°C\n",
      "  - Min Temp: 16.5°C\n",
      "  - Precipitation: 0.00 mm\n",
      "\n",
      "- **May 27, 2024**: \n",
      "  - Max Temp: 27.3°C\n",
      "  - Min Temp: 17.0°C\n",
      "  - Precipitation: 0.00 mm\n",
      "\n",
      "- **May 28, 2024**: \n",
      "  - Max Temp: 27.2°C\n",
      "  - Min Temp: 16.5°C\n",
      "  - Precipitation: 0.00 mm\n",
      "\n",
      "### Summary:\n",
      "- **Average Maximum Temperature**: 24.51°C\n",
      "- **Average Minimum Temperature**: 16.04°C\n",
      "- **Total Precipitation**: 24.40 mm\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for trace in trace_list:\n",
    "    print(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent_chain.invoke({\"input\": \"Which is more populous city, Budapest or Prague?\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agent_executor.invoke({\"input\": \"How many people live in Budapest and Prague together?\"})\n",
    "print(agent_chain.invoke({\"input\": \"How many people live in Budapest and Prague together?\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent_chain.invoke({\"input\": \"What is the ratio of inhabitants in percentage in Prague vs Budapest?\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent_chain.invoke({\"input\": \"What is the weather like in Budapest?\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent_chain.invoke({\"input\": \"What is the weather like in Prague?\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent_chain.invoke({\"input\": \"Where is the wether warmer currently, in Budapest or Prague?\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(agent_chain.invoke({\"input\": \"What are the 10 trending news in Budapest? Answer in 10 bullet points.\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent_chain.invoke({\"input\": \"What are the 10 trending news in Prague? Answer in 10 bullet points.\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent_chain.invoke({\"input\": \"Who won the most Oscar in this year?\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent_chain.invoke({\"input\": \"List the title of 10 scientific papers about LLM agents published in this year.\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
