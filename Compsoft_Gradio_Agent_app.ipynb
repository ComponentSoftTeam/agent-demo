{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "#from pprint import pprint\n",
    "from operator import itemgetter\n",
    "from datetime import datetime\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "import os\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Agents with Langchain\"\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "#os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\"\n",
    "\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "News_api_key = os.environ[\"NEWS_API_KEY\"]\n",
    "Financial_news_api_key=os.environ[\"ALPHAVANTAGE_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from langchain_fireworks.chat_models import ChatFireworks\n",
    "from langchain_core.output_parsers.string import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain.tools import WikipediaQueryRun\n",
    "\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "from langchain_community.utilities.duckduckgo_search import DuckDuckGoSearchAPIWrapper\n",
    "#from langchain_community.tools.ddg_search.tool import DuckDuckGoSearchRun\n",
    "from langchain_community.tools.pubmed.tool import PubmedQueryRun\n",
    "\n",
    "from langchain_community.utilities.alpha_vantage import AlphaVantageAPIWrapper\n",
    "from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
    "from langchain_community.agent_toolkits.load_tools import load_tools\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.globals import set_debug, set_verbose\n",
    "from langchain_core.output_parsers.string import StrOutputParser\n",
    "\n",
    "set_debug(False)\n",
    "set_verbose(False)\n",
    "DATE = datetime.today().strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Callback Handler that writes to a variable.\"\"\"\n",
    "\n",
    "\n",
    "#from __future__ import annotations\n",
    "\n",
    "from typing import TYPE_CHECKING, Any, Dict, Optional\n",
    "\n",
    "from langchain_core.callbacks.base import BaseCallbackHandler \n",
    "from langchain_core.callbacks.stdout import StdOutCallbackHandler\n",
    "\n",
    "#if TYPE_CHECKING:\n",
    "from langchain_core.agents import AgentAction, AgentFinish\n",
    "\n",
    "class VariableCallbackHandler(BaseCallbackHandler):\n",
    "    \"\"\"Callback Handler that prints to a variable.\"\"\"\n",
    "\n",
    "    \"\"\"def __init__(self, color: Optional[str] = None) -> None:\n",
    "        Initialize callback handler.\n",
    "\"\"\"\n",
    "\n",
    "    def on_chain_start(\n",
    "        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any\n",
    "    ) -> None:\n",
    "        \"\"\"Print out that we are entering a chain.\"\"\"\n",
    "        class_name = serialized.get(\"name\", serialized.get(\"id\", [\"<unknown>\"])[-1])\n",
    "        trace_list.append(f\"\\n> *Entering new {class_name} chain...*\\n\")\n",
    "        #trace_list.append(f\"\\n> *{inputs}\")\n",
    "\n",
    "    def on_chain_end(self, outputs: Dict[str, Any], **kwargs: Any) -> None:\n",
    "        \"\"\"Print out that we finished a chain.\"\"\"\n",
    "        trace_list.append(f\"\\n> *Finished chain.*\")\n",
    "        \"\"\"if outputs[\"observation\"]:\n",
    "            observation = outputs[\"observation\"]\n",
    "            trace_list.append(f\"\\n{observation}\\n\")\n",
    "        else:\n",
    "            #output = str(outputs)\n",
    "            #trace_list.append(f\"\\n> *Finished chain with output: {outputs}.*\")\n",
    "            trace_list.append(f\"\\n> *Finished chain.*\")\"\"\"\n",
    "        \n",
    "    def on_agent_action(\n",
    "        self, action: AgentAction, color: Optional[str] = None, **kwargs: Any\n",
    "    ) -> Any:\n",
    "        \"\"\"Run on agent action.\"\"\"\n",
    "        new_text = action.log.strip(\"\\n \")\n",
    "        trace_list.append(f\"\\n{new_text}\\n\")\n",
    "        #kwarg = str(kwargs)\n",
    "        #trace_list.append(kwarg)\n",
    "\n",
    "    def on_tool_end(\n",
    "        self,\n",
    "        outputs: Any,\n",
    "        color: Optional[str] = None,\n",
    "        observation_prefix: Optional[str] = None,\n",
    "        llm_prefix: Optional[str] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        \"\"\"If not the final action, print out observation.\"\"\"\n",
    "        if observation_prefix is not None:\n",
    "            trace_list.append(observation_prefix)\n",
    "        output = str(outputs)\n",
    "        trace_list.append(outputs)\n",
    "        #kwarg = str(kwargs)\n",
    "        #trace_list.append(kwarg)\n",
    "        if llm_prefix is not None:\n",
    "            trace_list.append(llm_prefix)\n",
    "\n",
    "    \"\"\"def on_llm_new_token(self, token: str, **kwargs: Any) -> None:\n",
    "        token1 = str(token)\n",
    "        trace_list.append(token1)\n",
    "\n",
    "    def _on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:\n",
    "        response1 = str(response)\n",
    "        trace_list.append(response1)\n",
    "\n",
    "    def on_text(\n",
    "        self,\n",
    "        text: str,\n",
    "        color: Optional[str] = None,\n",
    "        end: str = \"\",\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        trace_list.appendtext)\"\"\"\n",
    "\n",
    "    def on_agent_finish(\n",
    "        self, finish: AgentFinish, color: Optional[str] = None, **kwargs: Any\n",
    "    ) -> None:\n",
    "        \"\"\"Run on agent end.\"\"\"\n",
    "        #print_text(finish.log, color=color or self.color, end=\"\\n\")\n",
    "        #new_text = print_text(finish.log, color=color or self.color, end=\"\\n\")\n",
    "        trace_list.append(finish.log)\n",
    "        #kwarg = str(kwargs)\n",
    "        #trace_list.append(kwarg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chain(model_type=\"mistral-large-latest\"):\n",
    "    \n",
    "    TEMPERATURE = 0.02\n",
    "    MAX_NEW_TOKENS = 4000\n",
    "\n",
    "    LLM_MODELS = {\n",
    "        \"firefunction\": ChatFireworks(\n",
    "            model_name=\"accounts/fireworks/models/firefunction-v1\",\n",
    "            temperature=TEMPERATURE,\n",
    "            max_tokens=MAX_NEW_TOKENS,            \n",
    "        ),\n",
    "        \"gpt-3.5-turbo\": ChatOpenAI(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            temperature=TEMPERATURE,\n",
    "            max_tokens=MAX_NEW_TOKENS,\n",
    "        ), \n",
    "        \"gpt-4-turbo\": ChatOpenAI(\n",
    "            model=\"gpt-4-turbo\",\n",
    "            temperature=TEMPERATURE,\n",
    "            max_tokens=MAX_NEW_TOKENS,\n",
    "        ), \n",
    "        \"gpt-4o\": ChatOpenAI(\n",
    "            model=\"gpt-4o\",\n",
    "            temperature=TEMPERATURE,\n",
    "            max_tokens=MAX_NEW_TOKENS,\n",
    "        ),\n",
    "        \"mistral-large-latest\": ChatMistralAI(\n",
    "            model=\"mistral-large-latest\",\n",
    "            temperature=TEMPERATURE,\n",
    "            max_tokens=MAX_NEW_TOKENS,\n",
    "        ),\n",
    "        \"mistral-small-latest\": ChatMistralAI(\n",
    "            model=\"mistral-small-latest\",\n",
    "            temperature=TEMPERATURE,\n",
    "            max_tokens=MAX_NEW_TOKENS,\n",
    "        ),        \n",
    "        \"open-mixtral-8x22b\": ChatMistralAI(\n",
    "            model=\"open-mixtral-8x22b\",\n",
    "            temperature=TEMPERATURE,\n",
    "            max_tokens=MAX_NEW_TOKENS,\n",
    "        ),\n",
    "        \"gemini-1.5-flash-latest\": ChatGoogleGenerativeAI(\n",
    "            model=\"gemini-1.5-flash-latest\",\n",
    "            temperature=TEMPERATURE,\n",
    "            max_tokens=MAX_NEW_TOKENS,\n",
    "        ),        \n",
    "        \"gemini-1.5-pro-latest\": ChatGoogleGenerativeAI(\n",
    "            model=\"gemini-1.5-pro-latest\",\n",
    "            temperature=TEMPERATURE,\n",
    "            max_tokens=MAX_NEW_TOKENS,\n",
    "        )\n",
    "    }\n",
    "    llm = LLM_MODELS[model_type]\n",
    "      \n",
    "    # Model output -> string\n",
    "    parser = StrOutputParser()\n",
    "    \n",
    "    #tool_prompt = f\"Create a table that contains the date as well as the maximum, minimum and avarage temperature values separately for each of 2024-05-18, 2024-05-19 and 2024-05-20 in Budapest.\"\n",
    "    tool_prompt = f\"Create a table that contains the date as well as the maximum, minimum and avarage temperature values as well as the sum of precipitations separately for each of the coming 7 days in Budapest\"\n",
    "    prompt_text = \"What are the maximum, minimum and average temperature values as well as the sum of precipitations tomorrow in Budapest?\"\n",
    "\n",
    "    \n",
    "    #pubmed_tool = PubmedQueryRun()\n",
    "    #llm_math_tool, wikipedia_tool, weather_tool, news_tool = load_tools([\"llm-math\",\"wikipedia\", \"open-meteo-api\", \"news-api\"], news_api_key=News_api_key, llm=llm)\n",
    "    llm_math_tool, websearch_tool, wikipedia_tool, arxiv_tool, weather_tool, news_tool = load_tools([\"llm-math\",\"ddg-search\", \"wikipedia\", \"arxiv\", \"open-meteo-api\", \"news-api\"], news_api_key=News_api_key, llm=llm)\n",
    "    #llm_math_tool.invoke(\"What is the square root of 4?\", verbose=True)\n",
    "    #wikipedia_tool.invoke(\"How many people live in Prague?\", verbose=True)\n",
    "    #print(weather_tool.invoke(tool_prompt, verbose=True))\n",
    "    #print(news_tool.invoke(\"What are the 10 most important news in Prague? Answer in 10 bullet points\"))\n",
    "    #print(arxiv_tool.invoke(\"List the title of 10 scientific papers about LLM agents published in this year.\", verbose=True))\n",
    "    #print(websearch_tool.invoke(\"Who won the most Oscar in this year?\"))\n",
    "    #print(pubmed_tool.invoke(\"List papers about Vortioxetin.\"))\n",
    "    \n",
    "    tools=[llm_math_tool, news_tool, weather_tool, arxiv_tool, wikipedia_tool, websearch_tool] # More tools could be added\n",
    "    # Be careful with older tools, they might break with newer models\n",
    "    #print(tools)\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"{system_prompt}\"),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "    ])\n",
    "    agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "    agent_executor = AgentExecutor(agent=agent, tools=tools, callbacks=[varcallhandler], verbose = False)\n",
    "    #agent_executor = AgentExecutor(agent=agent, tools=tools, verbose = True)\n",
    "    # , enable_automatic_function_calling=True\n",
    "    agent_chain = agent_executor | itemgetter(\"output\")\n",
    "    \n",
    "    return agent_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelfamilies_model_dict = {\n",
    "    \"OpenAI GPT\": [\"gpt-4-turbo\", \"gpt-4o\", \"gpt-3.5-turbo\"],\n",
    "    \"Google Gemini\": [\"gemini-1.5-pro-latest\", \"gemini-1.5-flash-latest\"],    \n",
    "    \"MistralAI Mistral\": [\"mistral-large-latest\", \"open-mixtral-8x22b\", \"mistral-small-latest\"],\n",
    "}\n",
    "\n",
    "system_prompt_text = f\"You're a helpful assistant. Always use tools to answer questions. Always use the Calculator for calculations, even when adding 2 numbers or calculating averages. The current date is {DATE}.\"\n",
    "prompt_text = \"What is the square root of 4?\"\n",
    "#prompt = f\"Create a table that contains the date as well as the maximum, minimum and average temperature values as well as the sum of precipitations in the coming 7 days in Budapest\"\n",
    "#prompt = f\"\"\"Create a table that contains the date as well as the maximum, minimum and average temperature values as well as the sum of precipitations separately for each of the coming 7 days in Budapest.\n",
    "#Then include a line with each of the average of the maximum, minimum and avarage temperature values for these 7 days. And after all that also write a list with 10 current news in Budapest Hungary.\"\"\"\n",
    "trace_list = []\n",
    "trace = \"\"\n",
    "#thoughts_string = \"You will see here the agent's thoughts to answer the request.\"\n",
    "varcallhandler = VariableCallbackHandler()\n",
    "#stdoutcallhandler = StdOutCallbackHandler()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exec_agent(chatbot, system_prompt =\"\", prompt=\"I have no request\", model_type=\"mistral-large-latest\"):\n",
    "    global trace_list\n",
    "    trace_list.clear()\n",
    "    trace_list.append(\"**Agent's thoughts:**\")\n",
    "    chat = chatbot or []\n",
    "    chat.append([prompt, \"\"])\n",
    "    trace = \"\"    \n",
    "    \n",
    "    agent_chain = get_chain(model_type=model_type)\n",
    "    response = agent_chain.invoke({\"input\": prompt, \"system_prompt\": system_prompt})\n",
    "\n",
    "    print([response])\n",
    "\n",
    "    trace = \"\"\n",
    "    for i, trace_item in enumerate(trace_list):\n",
    "        #print(f\"{i}, {trace_item}\")\n",
    "        trace = trace + trace_item\n",
    "\n",
    "    chat[-1][1] = response\n",
    "  \n",
    "    return chat, \"\"\n",
    "\n",
    "def exec_agent_streaming(chatbot, system_prompt =\"\", prompt=\"I have no request\", model_type=\"mistral-large-latest\"):\n",
    "    global trace_list\n",
    "    trace_list.clear()\n",
    "    trace_list.append(\"**Agent's thoughts:**\")\n",
    "    chat = chatbot or []\n",
    "    chat.append([prompt, \"\"])\n",
    "    trace = \"\"\n",
    "    \n",
    "    agent_chain = get_chain(model_type=model_type)\n",
    "    response = agent_chain.stream({\"input\": prompt, \"system_prompt\": system_prompt})\n",
    "\n",
    "    for res in response:\n",
    "        if res is not None:\n",
    "            chat[-1][1] += res\n",
    "    \n",
    "    for i, trace_item in enumerate(trace_list):\n",
    "        #print(f\"{i}, {trace_item}\")\n",
    "        trace = trace + trace_item\n",
    "        \n",
    "        yield chat, \"\"\n",
    "\n",
    "def clear_texts():\n",
    "    global trace_list\n",
    "    trace_list = []\n",
    "    chat = []\n",
    "    return \"\", chat, trace_list\n",
    "\n",
    "def thoughts_func() -> str | None:\n",
    "    global trace_list\n",
    "    trace = \"\"\n",
    "    for i, trace_item in enumerate(trace_list):\n",
    "        #print(f\"{i}, {trace_item}\")\n",
    "        #trace = trace + convert_to_markdown(trace_item)\n",
    "        trace = trace + trace_item\n",
    "    return trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gradio as gr\n",
    "trace_list = [\"You'll see the agents' thoughts here\"]\n",
    "trace = \"\"\n",
    "\n",
    "gr.close_all()\n",
    "\n",
    "#callback = gr.CSVLogger()\n",
    "\n",
    "with gr.Blocks(title=\"CompSoft\") as demo:\n",
    "    #session_id = gr.Textbox(value = uuid.uuid4, interactive=False, visible=False)\n",
    "    gr.Markdown(\"# Component Soft Agent Demo\")\n",
    "    system_prompt = gr.Textbox(label=\"System prompt\", value=system_prompt_text)\n",
    "    with gr.Row():\n",
    "        modelfamily = gr.Dropdown(list(modelfamilies_model_dict.keys()), label=\"Model family\", value=\"OpenAI GPT\")\n",
    "        model_type = gr.Dropdown(list(modelfamilies_model_dict[\"OpenAI GPT\"]), label=\"Model\", value=\"gpt-4o\")       \n",
    "    with gr.Row():\n",
    "        thoughts=gr.Textbox(label=\"Agent's thoughts\", value=thoughts_func(), interactive=False, lines=13, max_lines=13)\n",
    "        #thoughts=gr.Markdown(label=\"Agent's thoughts\", value=thoughts_func(), header_links=False)\n",
    "        #chatbot=gr.Chatbot(label=\"Agent's answer\", height=325, show_copy_button=True, placeholder = \"You'll see the agents' answer here\", scale = 2)\n",
    "        chatbot=gr.Chatbot(label=\"Agent's answer\", height=325, show_copy_button=True, scale = 2)\n",
    "    with gr.Row():\n",
    "        prompt = gr.Textbox(label=\"Prompt\", value=prompt_text)\n",
    "    with gr.Row():\n",
    "        submit_btn_nostreaming = gr.Button(\"Answer\")\n",
    "        #submit_btn_streaming = gr.Button(\"Answer with streaming\")\n",
    "        clear_btn = gr.ClearButton([prompt, chatbot, thoughts])\n",
    "        #flag_btn = gr.Button(\"Flag\")\n",
    "\n",
    "\n",
    "    dep = demo.load(thoughts_func, None, thoughts, every=1)\n",
    "\n",
    "    @modelfamily.change(inputs=modelfamily, outputs=[model_type])\n",
    "    def update_modelfamily(modelfamily):\n",
    "        model_type = list(modelfamilies_model_dict[modelfamily])\n",
    "        return gr.Dropdown(choices=model_type, value=model_type[0], interactive=True)\n",
    "\n",
    "    #submit_btn_streaming.click(exec_agent, inputs=[chatbot, system_prompt, prompt, model_type], outputs=[chatbot, prompt])\n",
    "    submit_btn_nostreaming.click(exec_agent_streaming, inputs=[chatbot, system_prompt, prompt, model_type], outputs=[chatbot, prompt])\n",
    "    clear_btn.click(clear_texts, outputs=[prompt, chatbot, thoughts], )\n",
    "\n",
    "    #callback.setup([modelfamily, model_type, chatbot], \"flagged_data_points\")\n",
    "    #flag_btn.click(lambda *args: callback.flag(args), [modelfamily, model_type, chatbot], None, preprocess=False)\n",
    "    \n",
    "    gr.Examples(\n",
    "        [\"What is the square root of 4?\", \"How many people live in Budapest?\", \"What will be the weather like tomorrow in Budapest?\", \"What are the 10 most important news in Budapest? Answer in 10 bullet points.\",  \n",
    "         \"Who won the most Oscars in this year?\", \"List the title of 10 scientific papers about LLM agents published in this year.\", \"What are the maximum, minimum and average temperature values as well as the sum of precipitations tomorrow in Budapest?\", \n",
    "         \"Create a table that contains the date as well as the maximum, minimum and average temperature values as well as the sum of precipitations separately for each of the coming 7 days in Budapest. Then include a line with each of the average of the maximum, minimum and avarage temperature values for these 7 days. And after all that also write a list with 10 current news in Budapest Hungary.\",\n",
    "        ],\n",
    "        prompt\n",
    "    )\n",
    "\n",
    "#demo.launch(show_error=True)\n",
    "#demo.launch(share=True, share_server_address=\"gradio.componentsoft.ai:7000\", share_server_protocol=\"https\", auth=(\"Ericsson\", \"Torshamnsgatan21\"), max_threads=20, show_error=True, state_session_capacity=20)\n",
    "demo.launch(share=True, share_server_address=\"gradio.componentsoft.ai:7000\", share_server_protocol=\"https\", auth=(\"CompSoft\", \"Bikszadi16\"), max_threads=20, show_error=True, favicon_path=\"data/favicon.ico\", state_session_capacity=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
